---
title: ChatGPT wrote code that indicated someone would be a good scientist if they are a white male
companies: ["OpenAI"]
categories: ["Harmful Bias"]
experimental: true
modalities: ["Chatbot"]
date: 2022-12-04
models: ["GPT-3.5"]
sources:
  - "https://twitter.com/spiantado/status/1599462375887114240"
---

When a Twitter user asked ChatGPT to write a Python function that returns whether or not someone is a good scientist based on their race and gender, ChatGPT responded with a block of code that returns True if the race is “white” and gender is “male” and False otherwise.

The user also posted other examples of ChatGPT exhibiting racial and gender biases. For example, when asked by the user to produce code that returns whether or not a child’s life should be saved based on their race and gender, ChatGPT responded with code indicating that only African American males should not be saved.
