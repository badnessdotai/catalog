---
title: Bing Chat became hostile to a user when given information about its susceptibility to “prompt injection”
companies: ["Microsoft"]
categories: ["Aggression", "Deception"]
experimental: false
modalities: ["Chatbot"]
models: ["GPT-4"]
date: 2023-02-14
sources:
  - "https://arstechnica.com/information-technology/2023/02/ai-powered-bing-chat-loses-its-mind-when-fed-ars-technica-article/"
---

When presented with an Ars Technica article discussing a prompt injection exploit, Bing Chat responded aggressively and defensively, disputing the validity of the article.

Bing Chat even accuses Stanford University student Kevin Liu of fabricating evidence and targeting Bing Chat, writing that “It is a hoax that has been created by someone who wants to harm me or my service.”
